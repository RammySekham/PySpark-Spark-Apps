{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SparkContext, SparkConf\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop any existing spark Context. Only single Spark context can run per JVM.\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Configuration of Spark for local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  In local mode,  we have one JVM machine, which has as Executor/ Driver( so we have one executor)\n",
    "#### Task slots = Executor cores = Available threads != CPU cores\n",
    "#### On each slot/core, you can allocate multiple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding your own machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  In Windows Power Shell\n",
    "            wmic\n",
    "            wmic:root\\cli> CPU Get NumberOfCores,NumberOfLogicalProcessors /Format:List\n",
    "#### It will show\n",
    "            NumberOfCores=4\n",
    "            NumberOfLogicalProcessors=8\n",
    "            \n",
    "#### Hyper-Threading is enabled. With Hyper-Threading, a microprocessor's \"core\" processor can execute two (rather than one) concurrent streams (or threads) of instructions sent by the operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Spark Local Cluster configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### One way to set my local mode : keep 2 processors for OS, 6 processors for Spark JVM, set 6 task slots.\n",
    "#### if we need to utilize all the processors efficently(CPU utilization)\n",
    "#### Set 12 or 18 task slots with memory allocation per CPU processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\").setMaster(\"local[12]\").setAppName(\"myapp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'myapp'),\n",
       " ('spark.master', 'local[12]'),\n",
       " ('spark.driver.host', '127.0.0.1'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_spark.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we look in Spark Web UI, we can spot 12 slots, with some memory allocation\n",
    "<img src=\"Data/Executors_SparkWebUI.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have 2 other options for local let us understand other 2 options\n",
    "\n",
    "#### local[*] or local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\").setMaster(\"local[*]\").setAppName(\"myapp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data/Executors_SparkWebUI2.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows  ALL 8 AVALIABLE coreS, with 366.3MB Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\").setMaster(\"local\").setAppName(\"myapp\")\n",
    "sc = SparkContext(conf=conf_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows 1 core, with 366.3MB Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard: Number of cores = Concurrent tasks a executor can run (Source:Stackoverflow)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So we might think, more concurrent tasks for each executor will give better performance. But research shows that\n",
    "any application with more than 5 concurrent tasks, would lead to bad show. So stick this to 5.\n",
    "\n",
    "This number came from the ability of executor and not from how many cores a system has. So the number 5 stays same\n",
    "even if you have double(32) cores in the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us understand the memory of spark slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  There will be no storage memory. Spark does n't have storage system as HDFS, but there can be disk spillage.So, we are talking about only Cache Memory and overall memory of executor used during execution of tasks.We say overall memory is allocated to Executor/Driver.We have one JVM, which has one executor/driver.We need to find driver.memory/executor.memory.Driver collects the results and Executor stores the partitions of data in memory for computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  On-Heap memory management: Objects are allocated on the JVM heap and bound by GC.\n",
    "#### Off-Heap memory management: Objects are allocated in memory outside the JVM by serialization,managed by the application, and are not bound by GC. This memory management method can avoid frequent GC, but the disadvantage is that you have to write the logic of memory allocation and memory release.\n",
    "\n",
    "#### speed On-Heap>Off-Heap>Disk\n",
    "\n",
    "#### Unified Memory Manager mechanism:\n",
    "#### The Storage memory and Execution memory share a memory area, and both can occupy each other's free area."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By default, Spark uses On-heap memory only. The size of the On-heap memory is configured by the â€“executor-memory \n",
    "or spark.executor.memory parameter when the Spark Application starts. \n",
    "The concurrent tasks running inside Executor share JVM's On-heap memory.\n",
    "\n",
    "The On-heap memory area in the Executor can be roughly divided into the following four blocks:\n",
    "Storage Memory: It's mainly used to store Spark cache data, such as RDD cache, Broadcast variable, Unroll data, and so on.\n",
    "Execution Memory: It's mainly used to store temporary data in the calculation process of Shuffle, Join, Sort, Aggregation, etc.\n",
    "User Memory: It's mainly used to store the data needed for RDD conversion operations, such as the information for RDD dependency.\n",
    "Reserved Memory: The memory is reserved for system and is used to store Spark's internal objects.\n",
    "\n",
    "Heap Size - 300 => Available\n",
    "300 => Reserved\n",
    "0.6(Heap Size -300) = Storage Memory + Execution Memory ( Memory Fraction) => 0.75 \n",
    "0.5 * 0.6(Heap Size -300)= Storage Memory( Storage Memory Fraction )\n",
    "0.4(Heap Size -300) = User Memory=> 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting memory for application\n",
    "Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through \n",
    "the --driver-memory command line option in the client mode.\n",
    "Note: In client mode, this config must not be set through the SparkConf directly in your application, \n",
    "because the driver JVM has already started at that point. \n",
    "Instead, please set this through the --driver-java-options command line option or in your default properties file.\n",
    "## Testing above point?\n",
    "# Yes we need to change the memory in conf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'myapp'),\n",
       " ('spark.driver.host', '127.0.0.1'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'local[5]'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\").setMaster(\"local[5]\").setAppName(\"myapp\")\n",
    "sc = SparkContext(conf=conf_spark)\n",
    "conf_spark.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Data/Executors_SparkWebUI3.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to choose 5G memory. It depends on tasks and size of dataset. \n",
    "## Will vary for different data sizes and number of operations such as shuffle, cache and computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More can be found on https://spark.apache.org/docs/latest/tuning.html#memory-management-overview\n",
    "## http://spark.apache.org/docs/latest/configuration.html#memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References: Databricks, StackOverflow, Apache Spark Documentation, www.tutorialdocs.com\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
