{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading different types of Data, Partitions, Parellization\n",
    "##### DATAFRAME API IS PREFERRED against RDD API, as it is much faster. Datasets API is not avaliable in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataframes are immutable ; with every transformation new dataset is created\n",
    "\n",
    "#### 2. Spark datasets are represented as a list of entries.\n",
    "       This list is broken into partitions stored on a different machines. \n",
    "       Each partition holds a unique subset of the entries in the list. \n",
    "       Spark calls these datasets \"Resilient Distributed Datasets\" (RDDs).\n",
    "#### 3. At low level, everything is implemented as RDDs\n",
    "\n",
    "#### 4. DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    "\n",
    "#### 5.When you create a DataFrame, this collection is going to be parallelized\n",
    "\n",
    "#### 6.Spark DataFrames schemas are defined as a collection of typed columns. The entire schema is stored as a StructType and individual columns are stored as StructFields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "####  Hadoop format \n",
    "1. CSV Files\n",
    "2. Text Files\n",
    "3. JSON Records\n",
    "4. Avro Files\n",
    "5. Sequence Files\n",
    "6. RC Files\n",
    "7. ORC Files\n",
    "8. Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"fileformats\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text/CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "pandas_df = pd.read_csv(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                 int64\n",
       "city                       object\n",
       "city_development_index    float64\n",
       "gender                     object\n",
       "relevent_experience        object\n",
       "enrolled_university        object\n",
       "education_level            object\n",
       "major_discipline           object\n",
       "experience                 object\n",
       "company_size               object\n",
       "company_type               object\n",
       "last_new_job               object\n",
       "training_hours              int64\n",
       "target                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DataType\n",
    "            ArrayType\n",
    "            MapType\n",
    "            NullType\n",
    "            StructField\n",
    "            StructType\n",
    "    AtomicType(DataType)\n",
    "        BinaryType\n",
    "        BooleanType\n",
    "        DateType\n",
    "        StringType\n",
    "        TimestampType\n",
    "    FractionalType(NumericType)\n",
    "        DecimalType\n",
    "        DoubleType\n",
    "        FloatType\n",
    "    IntegralType(NumericType)\n",
    "        ByteType\n",
    "        IntegerType\n",
    "        LongType\n",
    "        ShortType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StructField objects are created with the name, dataType, and nullable properties\n",
    "schema = StructType([StructField(\"Customer\", StringType(), True)\\\n",
    "                   ,StructField(\"Month\", DateType(), True)\\\n",
    "                   ,StructField(\"Amount\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"enrollee_id\", IntegerType(), False)\\\n",
    "                    ,StructField(\"city\", StringType(), True)\\\n",
    "                    ,StructField(\"city_development_index\", FloatType(), True)\\\n",
    "                    ,StructField('gender', StringType(), True)\\\n",
    "                    ,StructField('relevent_experience', StringType(), True)\\\n",
    "                    ,StructField('enrolled_university', StringType(), True)\\\n",
    "                    ,StructField('education_level', StringType(), True)\\\n",
    "                    ,StructField('major_discipline', StringType(), True)\\\n",
    "                    ,StructField('experience', StringType(), True)\\\n",
    "                    ,StructField('company_size', StringType(), True)\\\n",
    "                    ,StructField('company_type', StringType(), True)\\\n",
    "                    ,StructField('last_new_job', StringType(), True)\\\n",
    "                    ,StructField('training_hours', IntegerType(), True),StructField('target', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.createDataFrame(pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2\n",
    "df2= (spark.read.format(\"csv\").options(header=\"true\").load(\"Data/Employee_Statistics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us compare method 2 & method 1\n",
    "## In Method 2: whole data is loaded as string\n",
    "## Method 1 give us more control over describing data types for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- enrollee_id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_development_index: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- relevent_experience: string (nullable = true)\n",
      " |-- enrolled_university: string (nullable = true)\n",
      " |-- education_level: string (nullable = true)\n",
      " |-- major_discipline: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      " |-- company_type: string (nullable = true)\n",
      " |-- last_new_job: string (nullable = true)\n",
      " |-- training_hours: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 give me default data partitions done while dataframe creation, but partitions are not created in method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
