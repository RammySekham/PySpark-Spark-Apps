{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading different types of Data, Partitions, Parellization\n",
    "##### DATAFRAME API IS PREFERRED against RDD API, as it is much faster. Datasets API is not avaliable in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataframes are immutable ; with every transformation new dataset is created\n",
    "\n",
    "#### 2. Spark datasets are represented as a list of entries.\n",
    "       This list is broken into partitions stored on a different machines. \n",
    "       Each partition holds a unique subset of the entries in the list. \n",
    "       Spark call these datasets \"Resilient Distributed Datasets\" (RDDs).\n",
    "#### 3. At low level, everything is implemented as RDDs\n",
    "\n",
    "#### 4. DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    "\n",
    "#### 5.When you create a DataFrame, this collection is going to be parallelized\n",
    "\n",
    "#### 6.Spark DataFrames schemas are defined as a collection of typed columns. The entire schema is stored as a StructType and individual columns are stored as StructFields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "####  Hadoop format \n",
    "1. CSV Files\n",
    "2. Text Files\n",
    "3. JSON Records\n",
    "4. Avro Files\n",
    "5. Sequence Files\n",
    "6. ORC Files\n",
    "7. Parquet Files\n",
    "8. XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are 3 different ways to create dataframes in pyspark\n",
    "    1. Read from data directly to CreateDataFrame\n",
    "    2. Create RDD and pass it to CreateDataFrame\n",
    "    3. Create pandas df and pass it to CreateDataFrame\n",
    "\n",
    "Differences in 1, 2 & 3\n",
    "Numofpartitions:In method 1, it is 1, In method 2, it is 2, In method 3, it is 8\n",
    "Method1 : Raw Data => Spark DataFrame #Best Method , spark takes less operations and less time to convert into internal mapRDD \n",
    "Method2 : Raw Data => RDD => Spark DataFrame\n",
    "Method3 : Raw Data => PandasDF => Spark DataFarme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"fileformats\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "pandas_df = pd.read_csv(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                 int64\n",
       "city                       object\n",
       "city_development_index    float64\n",
       "gender                     object\n",
       "relevent_experience        object\n",
       "enrolled_university        object\n",
       "education_level            object\n",
       "major_discipline           object\n",
       "experience                 object\n",
       "company_size               object\n",
       "company_type               object\n",
       "last_new_job               object\n",
       "training_hours              int64\n",
       "target                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DataType\n",
    "            ArrayType\n",
    "            MapType\n",
    "            NullType\n",
    "            StructField\n",
    "            StructType\n",
    "    AtomicType(DataType)\n",
    "        BinaryType\n",
    "        BooleanType\n",
    "        DateType\n",
    "        StringType\n",
    "        TimestampType\n",
    "    FractionalType(NumericType)\n",
    "        DecimalType\n",
    "        DoubleType\n",
    "        FloatType\n",
    "    IntegralType(NumericType)\n",
    "        ByteType\n",
    "        IntegerType\n",
    "        LongType\n",
    "        ShortType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to define Schema for pandas dataframe, \n",
    "## because Spark DataFrame can't infer spark dataframe schema from pandas dataframe\n",
    "## It may throw an error.\n",
    "\n",
    "## df= spark.createDataFrame(panda_df)\n",
    "\n",
    "## One of the error example when,I run above code line without giving schema \n",
    "\n",
    "##TypeError: field company_size: \n",
    "##        Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"enrollee_id\", IntegerType(), False)\\\n",
    "                    ,StructField(\"city\", StringType(), True)\\\n",
    "                    ,StructField(\"city_development_index\", FloatType(), True)\\\n",
    "                    ,StructField('gender', StringType(), True)\\\n",
    "                    ,StructField('relevent_experience', StringType(), True)\\\n",
    "                    ,StructField('enrolled_university', StringType(), True)\\\n",
    "                    ,StructField('education_level', StringType(), True)\\\n",
    "                    ,StructField('major_discipline', StringType(), True)\\\n",
    "                    ,StructField('experience', StringType(), True)\\\n",
    "                    ,StructField('company_size', StringType(), True)\\\n",
    "                    ,StructField('company_type', StringType(), True)\\\n",
    "                    ,StructField('last_new_job', StringType(), True)\\\n",
    "                    ,StructField('training_hours', IntegerType(), True),StructField('target', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.createDataFrame(pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2, Data is loaded as a spark dataframe( Not as RDD, not as Pandas Dataframe)\n",
    "df2= (spark.read.format(\"csv\").options(header=\"true\").load(\"Data/Employee_Statistics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us compare method 2 & method 1\n",
    "## In Method 2: whole data is loaded as string\n",
    "## Method 1 give us more control over describing data types for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 give me default data partitions done while dataframe creation, but partitions are not created in method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 3\n",
    "RDD_csv = spark.sparkContext.textFile(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.createDataFrame(RDD_csv, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Text File\n",
    "##### It will be similar to csv file. Let us see if we can spot any differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A create DataFRAME can take only three types of data, a list, pandas Dataframe or RDD\n",
    "## We have three options while reading data from external source\n",
    "## A pure text file(such as book page) can be read as pandas data frame or pure RDD or Spark DataFrame\n",
    "## Use Case: for NLP problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## an RDD of :class:`Row`/:class:`tuple`/:class:`list`/:class:`dict`,:class:`list`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_list = spark.sparkContext.textFile(\"Data/bookpage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us check this RDD_text looks like\n",
    "RDD_list.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us convert this RDD into dataframe\n",
    "\n",
    "## df3 = spark.createDataFrame(RDD_list)\n",
    "\n",
    "## when I run above line of code, it throws an error\n",
    "\n",
    "## TypeError: Can not infer schema for type: <class 'str'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =spark.createDataFrame(RDD_list, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us do with Method2: Pandasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_table(\"Data/bookpage.txt\", header=None, names=['PlainTextField'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 3: Loading data into Spark Dataframe \n",
    "Spark_Df =spark.read.text(\"Data/bookpage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark_Df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark_Df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1: Making Spark Dataframe by reading directly from Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=spark.read.json(\"Data\\sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas Dataframe\n",
    "pandas_df = pd.read_json(\"Data/sparkify_log_small.json\", lines=True)\n",
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when I run following line, I got type error, we have to describe schema to convert each pandas type to spark type.\n",
    "#df6 = spark.createDataFrame(pandas_df)\n",
    "\n",
    "#TypeError: field artist: \n",
    "        #Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save effort writing schema, i will go with create dierctly spark dataframe by reading raw data\n",
    "# But I have to make sure, I get parallelism as, spark offers for reading pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=spark.read.json(\"Data\\sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A DataFrame is already optimized for parallel execution, we need not to give it- number of partitions##\n",
    "## DataFrame is a distributed data structure. It is neither required nor possible to parallelize it. (source:stackoverflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Need to understand partitions in more detail:\n",
    "##### Spark uses Hadoop InputFilFormat under the hood, it will be reading partitions by input block (source:stackoverflow)\n",
    "##### Paritions are logical divisions of data from RDD (as input spilts are created from Hadoop blocks ). \n",
    "##### Hadoop default block size is 128MB, that is default partition size\n",
    "##### Spark uses map-reduce API to partition the data\n",
    "##### Slice Size = Maths.Max(minSize, Maths.min(maxSize, BlockSize)), we can alter max and min sizes of partitions\n",
    "##### data is divided into n number of partitions of Slice size\n",
    "##### By default blocksize is 128MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When the number of partitions is between 100 and 10K partitions\n",
    "## based on the size of the cluster and data, the lower and upper bound should be determined.\n",
    "\n",
    "## The lower bound for spark partitions is determined by 2 X number of cores in the cluster available to application.\n",
    "## Determining the upper bound for partitions in Spark, \n",
    "## the task should take 100+ ms time to execute. If it takes less time, \n",
    "## then the partitioned data might be too small or the application might be spending extra time in scheduling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can do repartition based on use cases, \n",
    "## but it involved shuffling, which  add additional network cost, so we need to careful about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AVRO file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro, a schema-based serialization technique.\n",
    "Avro serializes the data which has a built-in schema. \n",
    "Avro serializes the data into a compact binary format, which can be deserialized by any application.\n",
    "Avro uses JSON format to declare the data structures.\n",
    "Resulting serialized data is lesser in size. Schema is stored along with the Avro data in a file for any further processing.\n",
    "We have from Avro and to Avro functions in  pyspark.sql.avro.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversting Spark Dataframe into Avro file\n",
    "\n",
    "from pyspark.sql.avro.functions import to_avro\n",
    "\n",
    "##  to_avro Converts a column into binary of avro format., This is useful in case of Kafka\n",
    "## https://spark.apache.org/docs/3.0.0-preview/api/python/_modules/pyspark/sql/avro/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.avro.functions import to_avro\n",
    "data = ['SPADES']\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.select(to_avro(df.value).alias(\"suite\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing spark dataframe to Avro\n",
    "df1.write.format(\"avro\").save(\"Data/test3.avro\")\n",
    "\n",
    "## By default, avro files are written in same number of partitions as a data frame\n",
    "## if dataframe has 2 partitions, avro will also have 2, if data frame has 8, avro will have 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting avro file to Spark Dataframe\n",
    "df6 =spark.read.format(\"avro\").load(\"Data/test3.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## three types of compressions, when storing data on disk space\n",
    "## https://spark.apache.org/docs/3.0.0/sql-data-sources-avro.html\n",
    "\n",
    "## Compression codec used in writing of AVRO files.\n",
    "## Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.\n",
    "df1.write.format(\"avro\").option(\"compression\", \"deflate\").save(\"Data/test4.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PartitionBy in Arvo, we can partition data based on any column for saving on disk\n",
    "\n",
    "df6.write.partitionBy(\"major_discipline\").format(\"avro\").save(\"custom_partitioned.avro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data/Capture.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema in Avro\n",
    "## Avro schemas are usually defined with .avsc extension and the format of the file is in JSON.\n",
    "## We can provide this file using option() while reading an Avro file. \n",
    "## The schema provides the structure of the Avro file with field names and it’s data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SEQUQENCE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence files are one of the Apache Hadoop specific file formats which stores data in serialized key-value pair. Serialized in the sense: Stream of bytes.Hadoop Sequence file is a flat file structure which consists of serialized/binary key-value pairs. This is the same format in which the data is stored internally during the processing of the MapReduce tasks.\n",
    "\n",
    "What is the purpose of sequence file?\n",
    "\n",
    "1) To enable/store/process binary data\n",
    "\n",
    "2) The other objective of using SequenceFile is to pack many small files into a single large SequenceFile for the\n",
    "MapReduce computation since the design of Hadoop prefer large files. Sequence file also work well as containers for\n",
    "smaller files. HDFS and MapReduce are optimized for large files, so packing small files into a sequencefile makes storing\n",
    "and processing the smaller files more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to create a sequence file and writing a squence file, reading a sequence file . Unserstand structure\n",
    "## Sequence File => Spark DataFrame => Sequence File # No DataFrameAPI for sequence file \n",
    "## We can use RDD API sc.sequencefile to read sequence file and RDD.Saveassequence file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SequenceFileRDDFunctions\n",
    " \t\n",
    "saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
    "Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop file system, using the org.apache.hadoop.io.Writable types that we convert from the RDD's key and value types\n",
    " \t\n",
    "saveAsPickleFile(self, path, batchSize=10)\n",
    "Save this RDD as a SequenceFile of serialized objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_csv = spark.sparkContext.textFile(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## While running this code:\n",
    "## RDD_csv.saveAsSequenceFile(\"Data\\squenceTest\")\n",
    "## I got error :\n",
    "# Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n",
    "# : org.apache.spark.SparkException: RDD element of type java.lang.String cannot be used\n",
    "\n",
    "## So I need to convert my data into key-value pairs to save a sequence file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we think us ecase of sequence files, it is collectionn of small files\n",
    "RDD_text = spark.sparkContext.textFile(\"Datasets_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD = RDD_text.map(lambda x:(None,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD.saveAsSequenceFile(\"dataseq_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading sequence file\n",
    "RDD_seq = spark.sparkContext.sequenceFile(\"dataseq_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ORC Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ORC stands for Optimized Row Columnar which means it can store data in an optimized way than the other file formats. \n",
    "\n",
    "2. ORC reduces the size of the original data up to 75%. As a result the speed of data processing also increases and shows better performance than Text, Sequence and RC file formats. \n",
    "\n",
    "3. An ORC file contains rows data in groups called as Stripes along with a file footer.\n",
    "\n",
    "4. ORC File format provides very efficient way to store relational data.\n",
    "\n",
    "5. By using ORC File format we can reduce the size of original data up to 75%.( source: Nxt Gen)\n",
    "\n",
    "6. ORC takes less time to access the data and ORC takes Less space to store data. \n",
    "\n",
    "7. However, the ORC file increases CPU overhead by increasing the time it takes to decompress the relational data\n",
    "\n",
    "(Source: HortWorks Data Summit 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data/DataWorksSummit_ORC.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create ORC file from Spark Datafrmae.\n",
    "df1.write.format(\"orc\").save(\"Data\\orcfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"orc\").load(\"Data\\orcfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When querying, in this columnar storage you can skip over the non-relevant data very quickly,support advanced nested data structures.The layout of Parquet data files is optimized for queries that process large volumes of data, in the gigabyte range \n",
    "Most of the cloud companies, charge based on the amount of data scanned per query and amount of data stored. Parquet can be cost-saviour (source :Databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data/parquat_dist.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create Parquat file from Spark Datafrmae.\n",
    "df1.write.format(\"parquet\").save(\"Data\\parfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"Data\\parfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which one is best ? ORC, PARQUAT OR AVRO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. GENERAL RULE \n",
    "\n",
    "In general, if the data is wide, has a large number of attributes and is write-heavy, \n",
    "then a row-based approach may be best. (**AVRO**)\n",
    "If the data is narrower, \n",
    "has a fewer number of attributes, and is read-heavy, then a column-based approach may be best\n",
    "(**PARQUET OR ORC** depend on platform you use)\n",
    "\n",
    "###### 2. Read speed : ORC>AVRO>Parquat>JSON (depend on use cases)\n",
    "   Grabage Collection: Parquat>ORC>Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. XML DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
