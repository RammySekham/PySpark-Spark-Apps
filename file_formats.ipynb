{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading different types of Data, Partitions, Parellization\n",
    "##### DATAFRAME API IS PREFERRED against RDD API, as it is much faster. Datasets API is not avaliable in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataframes are immutable ; with every transformation new dataset is created\n",
    "\n",
    "#### 2. Spark datasets are represented as a list of entries.\n",
    "       This list is broken into partitions stored on a different machines. \n",
    "       Each partition holds a unique subset of the entries in the list. \n",
    "       Spark calls these datasets \"Resilient Distributed Datasets\" (RDDs).\n",
    "#### 3. At low level, everything is implemented as RDDs\n",
    "\n",
    "#### 4. DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    "\n",
    "#### 5.When you create a DataFrame, this collection is going to be parallelized\n",
    "\n",
    "#### 6.Spark DataFrames schemas are defined as a collection of typed columns. The entire schema is stored as a StructType and individual columns are stored as StructFields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "####  Hadoop format \n",
    "1. CSV Files\n",
    "2. Text Files\n",
    "3. JSON Records\n",
    "4. Avro Files\n",
    "5. Sequence Files\n",
    "6. RC Files\n",
    "7. ORC Files\n",
    "8. Parquet Files\n",
    "9. XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are 3 different ways to create dataframes in pyspark\n",
    "    1. Read from data directly to CreateDataFrame\n",
    "    2. Create RDD and pass it to CreateDataFrame\n",
    "    3. Create pandas df and pass it to CreateDataFrame\n",
    "\n",
    "Differences in 1, 2 & 3\n",
    "Numofpartitions:In method 1, it is 1, In method 2, it is 2, In method 3, it is 8\n",
    "Method1 : Raw Data => Spark DataFrame\n",
    "Method2 : Raw Data => RDD => Spark DataFrame\n",
    "Method3 : Raw Data => PandasDF => Spark DataFarme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().appName(\"fileformats\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "pandas_df = pd.read_csv(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                 int64\n",
       "city                       object\n",
       "city_development_index    float64\n",
       "gender                     object\n",
       "relevent_experience        object\n",
       "enrolled_university        object\n",
       "education_level            object\n",
       "major_discipline           object\n",
       "experience                 object\n",
       "company_size               object\n",
       "company_type               object\n",
       "last_new_job               object\n",
       "training_hours              int64\n",
       "target                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DataType\n",
    "            ArrayType\n",
    "            MapType\n",
    "            NullType\n",
    "            StructField\n",
    "            StructType\n",
    "    AtomicType(DataType)\n",
    "        BinaryType\n",
    "        BooleanType\n",
    "        DateType\n",
    "        StringType\n",
    "        TimestampType\n",
    "    FractionalType(NumericType)\n",
    "        DecimalType\n",
    "        DoubleType\n",
    "        FloatType\n",
    "    IntegralType(NumericType)\n",
    "        ByteType\n",
    "        IntegerType\n",
    "        LongType\n",
    "        ShortType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to define Schema for pandas dataframe, \n",
    "## because Spark DataFrame can't infer spark dataframe schema from pandas dataframe\n",
    "## It may throw an error.\n",
    "\n",
    "## df= spark.createDataFrame(panda_df)\n",
    "\n",
    "## One of the error example when,I run above code line without giving schema \n",
    "\n",
    "##TypeError: field company_size: \n",
    "##        Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"enrollee_id\", IntegerType(), False)\\\n",
    "                    ,StructField(\"city\", StringType(), True)\\\n",
    "                    ,StructField(\"city_development_index\", FloatType(), True)\\\n",
    "                    ,StructField('gender', StringType(), True)\\\n",
    "                    ,StructField('relevent_experience', StringType(), True)\\\n",
    "                    ,StructField('enrolled_university', StringType(), True)\\\n",
    "                    ,StructField('education_level', StringType(), True)\\\n",
    "                    ,StructField('major_discipline', StringType(), True)\\\n",
    "                    ,StructField('experience', StringType(), True)\\\n",
    "                    ,StructField('company_size', StringType(), True)\\\n",
    "                    ,StructField('company_type', StringType(), True)\\\n",
    "                    ,StructField('last_new_job', StringType(), True)\\\n",
    "                    ,StructField('training_hours', IntegerType(), True),StructField('target', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.createDataFrame(pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2, Data is loaded as a spark dataframe( Not as RDD, not as Pandas Dataframe)\n",
    "df2= (spark.read.format(\"csv\").options(header=\"true\").load(\"Data/Employee_Statistics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us compare method 2 & method 1\n",
    "## In Method 2: whole data is loaded as string\n",
    "## Method 1 give us more control over describing data types for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- enrollee_id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- city_development_index: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- relevent_experience: string (nullable = true)\n",
      " |-- enrolled_university: string (nullable = true)\n",
      " |-- education_level: string (nullable = true)\n",
      " |-- major_discipline: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      " |-- company_type: string (nullable = true)\n",
      " |-- last_new_job: string (nullable = true)\n",
      " |-- training_hours: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1 give me default data partitions done while dataframe creation, but partitions are not created in method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(enrollee_id=8949, city='city_103, city_103', city_development_index=0.9200000166893005, gender='Male', relevent_experience='Has relevent experience', enrolled_university='no_enrollment', education_level='Graduate', major_discipline='STEM', experience='>20', company_size='NaN', company_type='NaN', last_new_job='1', training_hours=36, target=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 3\n",
    "RDD_csv = spark.sparkContext.textFile(\"Data/Employee_Statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= spark.createDataFrame(RDD_csv, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Text File\n",
    "##### It will be similar to csv file. Let us see if we can spot any differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A create DataFRAME can take only three types of data, a list, pandas Dataframe or RDD\n",
    "## We have three options while reading data from external source\n",
    "## A pure text file(such as book page) can be read as pandas data frame or pure RDD or Spark DataFrame\n",
    "## Use Case: for NLP problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## an RDD of :class:`Row`/:class:`tuple`/:class:`list`/:class:`dict`,:class:`list`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_list = spark.sparkContext.textFile(\"Data/bookpage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fine for running, but does that idea hold for any pursuit?',\n",
       " 'Kriegel continues: “The same is true elsewhere: Trying easy',\n",
       " 'will help you in any area of your life. Conventional Wisdom',\n",
       " 'tells us we have to give no less than 110 percent to keep',\n",
       " 'ahead. Yet conversely, I have found that giving 90 percent is',\n",
       " 'usually more effective.”',\n",
       " 'For freewriting, too, Kriegel’s “easy” notion hits the nail',\n",
       " 'on its relaxed head.',\n",
       " 'Rather than approach your writing with your teeth gritted, demanding instant, virtuoso solutions from yourself,',\n",
       " 'loosen up and ease into your best 90 percent effort. Here’s']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let us check this RDD_text looks like\n",
    "RDD_list.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us convert this RDD into dataframe\n",
    "\n",
    "## df3 = spark.createDataFrame(RDD_list)\n",
    "\n",
    "## when I run above line of code, it throws an error\n",
    "\n",
    "## TypeError: Can not infer schema for type: <class 'str'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =spark.createDataFrame(RDD_list, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='Fine for running, but does that idea hold for any pursuit?')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let us do with Method2: Pandasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_table(\"Data/bookpage.txt\", header=None, names=['PlainTextField'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlainTextField</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fine for running, but does that idea hold for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kriegel continues: “The same is true elsewhere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will help you in any area of your life. Conven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tells us we have to give no less than 110 perc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahead. Yet conversely, I have found that givin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>usually more effective.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For freewriting, too, Kriegel’s “easy” notion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on its relaxed head.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rather than approach your writing with your te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loosen up and ease into your best 90 percent e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      PlainTextField\n",
       "0  Fine for running, but does that idea hold for ...\n",
       "1  Kriegel continues: “The same is true elsewhere...\n",
       "2  will help you in any area of your life. Conven...\n",
       "3  tells us we have to give no less than 110 perc...\n",
       "4  ahead. Yet conversely, I have found that givin...\n",
       "5                           usually more effective.”\n",
       "6  For freewriting, too, Kriegel’s “easy” notion ...\n",
       "7                               on its relaxed head.\n",
       "8  Rather than approach your writing with your te...\n",
       "9  loosen up and ease into your best 90 percent e..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PlainTextField='Fine for running, but does that idea hold for any pursuit?')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 3: Loading data into Spark Dataframe \n",
    "Spark_Df =spark.read.text(\"Data/bookpage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spark_Df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1: Making Spark Dataframe by reading directly from Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=spark.read.json(\"Data\\sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts                 int64\n",
       "userId            object\n",
       "sessionId          int64\n",
       "page              object\n",
       "auth              object\n",
       "method            object\n",
       "status             int64\n",
       "level             object\n",
       "itemInSession      int64\n",
       "location          object\n",
       "userAgent         object\n",
       "lastName          object\n",
       "firstName         object\n",
       "registration     float64\n",
       "gender            object\n",
       "artist            object\n",
       "song              object\n",
       "length           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Pandas Dataframe\n",
    "pandas_df = pd.read_json(\"Data/sparkify_log_small.json\", lines=True)\n",
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when I run following line, I got type error, we have to describe schema to convert each pandas type to spark type.\n",
    "#df6 = spark.createDataFrame(pandas_df)\n",
    "\n",
    "#TypeError: field artist: \n",
    "        #Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save effort writing schema, i will go with create dierctly spark dataframe by reading raw data\n",
    "# But I have to make sure, I get parallelism as, spark offers for reading pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=spark.read.json(\"Data\\sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A DataFrame is already optimized for parallel execution, we need not to give it- number of partitions##\n",
    "## DataFrame is a distributed data structure. It is neither required nor possible to parallelize it. (source:stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to understand partitions in more detail:\n",
    "## Spark uses Hadoop InputFilFormat under the hood, it will be reading partitions by input block (source:stackoverflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
